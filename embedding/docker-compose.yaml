services:
  tei:
    image: ghcr.io/huggingface/text-embeddings-inference:89-1.8 # change depending on architecture
    ports:
      - "9622:80"
    volumes:
      - ./app:/app
      - ./models:/data
    # pooling はモデルアーキテクチャにあったものに変える
    command: [ "--model-id", "cl-nagoya/ruri-v3-30m", "--dtype", "float16", "--pooling", "mean", "--max-batch-tokens", "131072", "--max-client-batch-size", "16" ]
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      # - HF_API_TOKEN=${HF_API_TOKEN}
    restart: always
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["0"]
              capabilities: [gpu]
